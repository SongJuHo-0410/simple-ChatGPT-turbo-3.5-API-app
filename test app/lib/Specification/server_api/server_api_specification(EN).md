# Server API

This repository contains the `server_api.py` file, which implements a server-side API using Flask for handling chat interactions with the GPT-3.5-turbo model from OpenAI.

## Dependencies

The following dependencies are required to run the server:

- `json`: Library for JSON data processing
- `Flask`: Web application framework
- `openai`: Library for interacting with the OpenAI API
- `os`: Library for environment variable configuration

## Configuration

Before running the server, make sure to set the OpenAI API key. You can set it in the code by assigning the key to the `openai.api_key` variable.

## Endpoints

### POST /chat

This endpoint allows clients to send chat messages and receive responses from the GPT-3.5-turbo model.

#### Request

- Headers:
  - `Content-Type: application/json`

- Body:
  - `system` (String): The system input message.
  - `user` (String): The user input message.

#### Response

- Body:
  - `answer` (String): The response answer generated by the model.
  - `summary` (String): The summary of the conversation generated by the model.

## Example Usage

To use the API, send a POST request to the `/chat` endpoint with the system and user input messages in the request body. The server will return the generated answer and summary in the response.

```bash
POST /chat
Content-Type: application/json

{
  "system": "System message",
  "user": "User message"
}
